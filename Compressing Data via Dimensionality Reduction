1.	feature extraction的優點
減少維度、提升模型效能、數據清洗、減少多元共線性1.	feature extraction的優點
減少維度、提升模型效能、數據清洗、減少多元共線性

2.	請介紹PCA
目標：通過線性變換將原始高維數據轉換為較低維度的數據，同時最大程度地保留數據的變異性。
主成分：主成分是新的、相互正交的基向量。每個主成分都是原始特徵的線性組合，並且依次捕獲數據中最大變異的方向。
協方差矩陣：PCA計算數據的協方差矩陣，然後對其進行特徵值分解或奇異值分解
3.	PCA的概念
4.	CPA的六個步驟
5.	請解釋variance ratio
解釋變異率（Variance Ratio），也稱為解釋方差比例，是一個重要的指標，用來衡量每個主成分對於總變異的貢獻程度。解釋變異率可以幫助我們理解每個主成分的重要性，並決定選擇多少個主成分來保留大部分的數據信息。
 
6.	請介紹covariance Matrix
協方差矩陣（Covariance Matrix）是多變量統計分析中的一個基本概念，用來描述多個變量之間的線性關係。如果協方差為正，說明這兩個變量正相關；如果協方差為負，說明這兩個變量負相關；如果協方差為零，說明這兩個變量無線性相關性。
 

7.	請問LDA是什麼?
線性判別分析（Linear Discriminant Analysis，簡稱LDA）是一種常用的降維技術，主要應用於分類問題。LDA通過尋找能夠最大化類間距離和最小化類內距離的投影方向，同時保留數據的判別信息來提高分類模型的性能。






8.	請問LDA的優缺點是什麼?
9.	請問LDA的假設是什麼?
假設有助於LDA在高維數據中找到能夠最大化類間距離和最小化類內距離的投影方向。
1.	Normal Distribution
2.	同協方差矩陣（Equal Covariance Matrices
3.	類別之間的獨立性（Independence of Observations）
10.	核主成分分析（Kernel Principal Component Analysis，Kernel PCA）是主成分分析（PCA）的非線性擴展。通過引入核技巧（Kernel Trick），核PCA可以在高維或無限維的特徵空間中執行PCA，從而能夠捕捉數據的非線性結構。
 
11.	Kernal PCA的優點
 

12.	Kernal PCA的缺點
 
13.	使用kernalPCA常見使用的kernal function
 
14.	非線性特徵組合來取代點積是通過引入核函數
在核主成分分析（Kernel PCA）中，使用非線性特徵組合來取代點積是通過引入核函數（Kernel Function）來實現的。這一方法允許我們在高維甚至無限維的特徵空間中操作，而無需顯式計算這些高維空間的坐標。

